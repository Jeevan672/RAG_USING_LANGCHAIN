{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-12T06:58:05.312891Z","iopub.execute_input":"2024-03-12T06:58:05.313202Z","iopub.status.idle":"2024-03-12T06:58:06.176261Z","shell.execute_reply.started":"2024-03-12T06:58:05.313175Z","shell.execute_reply":"2024-03-12T06:58:06.175346Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/ai-pdf/Joseph Babcock Raghav Bali - Generative AI with Python and TensorFlow 2_ Create images text and music with VAEs GANs LSTMs Transformer models-Packt Publishing (2021).pdf\n/kaggle/input/ai-pdf/Ben Auffarth - Generative AI with LangChain_ Build large language model (LLM) apps with Python ChatGPT and other LLMs-Packt (2023).pdf\n/kaggle/input/ai-pdf/Chris Fregly Antje Barth Shelbee Eigenbrode - Generative AI on AWS_ Building Context-Aware Multimodal Reasoning Applications-OReilly Media Inc. (2023).pdf\n/kaggle/input/ai-pdf/Python for Programmers with Big Data and Artificial Intelligence Case Studies by Paul J. Deitel Harvey Deitel (z-lib.org).pdf\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_HF = user_secrets.get_secret(\"Huggingface\")\nsecret_value_OA = user_secrets.get_secret(\"OPEN_API_KEY\")\nsecret_value_PC = user_secrets.get_secret(\"PINECONE\")\nsecret_value_Wbd = user_secrets.get_secret(\"wandb login\")\nsecret_value_A_KEY = user_secrets.get_secret(\"ASTRA_API_KEY\")\nsecret_value_A_DB = user_secrets.get_secret(\"ASTRA_DB_ID\")\nsecret_value_P_ENV = user_secrets.get_secret(\"PINECONE_ENV\")","metadata":{"execution":{"iopub.status.busy":"2024-03-12T09:18:17.978875Z","iopub.execute_input":"2024-03-12T09:18:17.979215Z","iopub.status.idle":"2024-03-12T09:18:19.435325Z","shell.execute_reply.started":"2024-03-12T09:18:17.979189Z","shell.execute_reply":"2024-03-12T09:18:19.434481Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"!pip install langchain","metadata":{"execution":{"iopub.status.busy":"2024-03-12T07:09:50.400571Z","iopub.execute_input":"2024-03-12T07:09:50.400887Z","iopub.status.idle":"2024-03-12T07:10:05.671090Z","shell.execute_reply.started":"2024-03-12T07:09:50.400862Z","shell.execute_reply":"2024-03-12T07:10:05.670140Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting langchain\n  Downloading langchain-0.1.11-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\nCollecting langchain-community<0.1,>=0.0.25 (from langchain)\n  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\nCollecting langchain-core<0.2,>=0.1.29 (from langchain)\n  Downloading langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\nCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.23-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.29->langchain) (4.2.0)\nCollecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.29->langchain)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.2.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nDownloading langchain-0.1.11-py3-none-any.whl (807 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.30-py3-none-any.whl (256 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\nDownloading langsmith-0.1.23-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, orjson, langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.1.11 langchain-community-0.0.27 langchain-core-0.1.30 langchain-text-splitters-0.0.1 langsmith-0.1.23 orjson-3.9.15 packaging-23.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-12T07:10:51.517772Z","iopub.execute_input":"2024-03-12T07:10:51.518118Z","iopub.status.idle":"2024-03-12T07:10:51.522604Z","shell.execute_reply.started":"2024-03-12T07:10:51.518093Z","shell.execute_reply":"2024-03-12T07:10:51.521497Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#!pip install PyPDFLoader","metadata":{"execution":{"iopub.status.busy":"2024-03-12T07:18:16.761863Z","iopub.execute_input":"2024-03-12T07:18:16.762185Z","iopub.status.idle":"2024-03-12T07:18:17.799282Z","shell.execute_reply.started":"2024-03-12T07:18:16.762161Z","shell.execute_reply":"2024-03-12T07:18:17.798609Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement PyPDFLoader (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for PyPDFLoader\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.document_loaders import PyPDFLoader\nloaders=[\n    #PyPDFLoader('/kaggle/input/ai-pdf/Ben Auffarth - Generative AI with LangChain_ Build large language model (LLM) apps with Python ChatGPT and other LLMs-Packt (2023).pdf'),\n    PyPDFLoader('/kaggle/input/ai-pdf/Chris Fregly Antje Barth Shelbee Eigenbrode - Generative AI on AWS_ Building Context-Aware Multimodal Reasoning Applications-OReilly Media Inc. (2023).pdf'),\n    #PyPDFLoader('/kaggle/input/ai-pdf/Joseph Babcock Raghav Bali - Generative AI with Python and TensorFlow 2_ Create images text and music with VAEs GANs LSTMs Transformer models-Packt Publishing (2021).pdf'),\n    #PyPDFLoader('/kaggle/input/ai-pdf/Python for Programmers with Big Data and Artificial Intelligence Case Studies by Paul J. Deitel Harvey Deitel (z-lib.org).pdf')\n    \n]\n\ndocs=[]\nfor loader in loaders:\n    docs.extend(loader.load())","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:50:40.783517Z","iopub.execute_input":"2024-03-12T10:50:40.783835Z","iopub.status.idle":"2024-03-12T10:50:44.049389Z","shell.execute_reply.started":"2024-03-12T10:50:40.783811Z","shell.execute_reply":"2024-03-12T10:50:44.048490Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter=RecursiveCharacterTextSplitter(\n    chunk_size=10000,\n    chunk_overlap=1000\n)\n\nsplits=text_splitter.split_documents(docs)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:50:44.050962Z","iopub.execute_input":"2024-03-12T10:50:44.051208Z","iopub.status.idle":"2024-03-12T10:50:44.078844Z","shell.execute_reply.started":"2024-03-12T10:50:44.051187Z","shell.execute_reply":"2024-03-12T10:50:44.078199Z"},"trusted":true},"execution_count":169,"outputs":[]},{"cell_type":"code","source":"#pip install openai","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:50:51.413565Z","iopub.execute_input":"2024-03-12T10:50:51.413881Z","iopub.status.idle":"2024-03-12T10:50:51.417775Z","shell.execute_reply.started":"2024-03-12T10:50:51.413856Z","shell.execute_reply":"2024-03-12T10:50:51.416925Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.embeddings.openai import OpenAIEmbeddings\nembeddings=OpenAIEmbeddings(openai_api_key=secret_value_OA)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:50:54.357118Z","iopub.execute_input":"2024-03-12T10:50:54.357437Z","iopub.status.idle":"2024-03-12T10:50:54.380180Z","shell.execute_reply.started":"2024-03-12T10:50:54.357397Z","shell.execute_reply":"2024-03-12T10:50:54.379326Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"\"\"\"from langchain.vectorstores import Chroma\nvectordb=Chroma.from_documents(\n    documents=splits,\n    embedding=embeddings,\n    persist_directory='RAG_BOT'\n)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:50:56.856284Z","iopub.execute_input":"2024-03-12T10:50:56.856618Z","iopub.status.idle":"2024-03-12T10:50:56.862788Z","shell.execute_reply.started":"2024-03-12T10:50:56.856593Z","shell.execute_reply":"2024-03-12T10:50:56.861715Z"},"trusted":true},"execution_count":172,"outputs":[{"execution_count":172,"output_type":"execute_result","data":{"text/plain":"\"from langchain.vectorstores import Chroma\\nvectordb=Chroma.from_documents(\\n    documents=splits,\\n    embedding=embeddings,\\n    persist_directory='RAG_BOT'\\n)\\n\""},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pip install FAISS-cpu\n","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:50:57.784875Z","iopub.execute_input":"2024-03-12T10:50:57.785186Z","iopub.status.idle":"2024-03-12T10:50:57.789032Z","shell.execute_reply.started":"2024-03-12T10:50:57.785161Z","shell.execute_reply":"2024-03-12T10:50:57.788013Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"code","source":"#pip install tiktoken","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:50:58.828156Z","iopub.execute_input":"2024-03-12T10:50:58.828511Z","iopub.status.idle":"2024-03-12T10:50:58.832793Z","shell.execute_reply.started":"2024-03-12T10:50:58.828486Z","shell.execute_reply":"2024-03-12T10:50:58.831933Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"markdown","source":"!pip install -qU \\\n pinecone-client==3.0.0","metadata":{"execution":{"iopub.status.busy":"2024-03-12T08:05:58.658133Z","iopub.execute_input":"2024-03-12T08:05:58.658491Z","iopub.status.idle":"2024-03-12T08:06:08.872595Z","shell.execute_reply.started":"2024-03-12T08:05:58.658461Z","shell.execute_reply":"2024-03-12T08:06:08.871753Z"}}},{"cell_type":"code","source":"from langchain.vectorstores import FAISS\n\nvectordb=FAISS.from_documents(splits,embeddings)\nvectordb.save_local('RAG_BOT_')","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:51:04.915175Z","iopub.execute_input":"2024-03-12T10:51:04.915501Z","iopub.status.idle":"2024-03-12T10:51:10.308373Z","shell.execute_reply.started":"2024-03-12T10:51:04.915478Z","shell.execute_reply":"2024-03-12T10:51:10.307463Z"},"trusted":true},"execution_count":175,"outputs":[]},{"cell_type":"code","source":"from langchain.chat_models import ChatOpenAI\nllm=ChatOpenAI(openai_api_key=secret_value_OA,model_name=\"gpt-3.5-turbo\",temperature=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:51:10.311709Z","iopub.execute_input":"2024-03-12T10:51:10.311943Z","iopub.status.idle":"2024-03-12T10:51:10.333261Z","shell.execute_reply.started":"2024-03-12T10:51:10.311923Z","shell.execute_reply":"2024-03-12T10:51:10.332471Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\ntemplate=\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n{context}\nQuestion:{question}\nHelpful Answer:\"\"\"\nQA_CHAIN_PROMPT=PromptTemplate(input_variables=[\"context\",\"Question\"],template=template)\n\n#run chain\nfrom langchain.chains import RetrievalQA\nqa_chain=RetrievalQA.from_chain_type(llm,\n                                     retriever=vectordb.as_retriever(),\n                                     return_source_documents=True,\n                                     chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})#search_type=\"mmr\"","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:51:10.960194Z","iopub.execute_input":"2024-03-12T10:51:10.961037Z","iopub.status.idle":"2024-03-12T10:51:10.967056Z","shell.execute_reply.started":"2024-03-12T10:51:10.961010Z","shell.execute_reply":"2024-03-12T10:51:10.965895Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"code","source":"from langchain.memory import ConversationBufferMemory\nmemory=ConversationBufferMemory(\n    memory_key=\"chat_history\",\n    return_messages=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:51:15.907349Z","iopub.execute_input":"2024-03-12T10:51:15.907924Z","iopub.status.idle":"2024-03-12T10:51:15.912139Z","shell.execute_reply.started":"2024-03-12T10:51:15.907901Z","shell.execute_reply":"2024-03-12T10:51:15.911517Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"code","source":"\"\"\"from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n\nconversation = ConversationChain(\n\tllm=llm,\n\tmemory=ConversationBufferWindowMemory(k=1)\n)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:51:45.926497Z","iopub.execute_input":"2024-03-12T10:51:45.927311Z","iopub.status.idle":"2024-03-12T10:51:45.933235Z","shell.execute_reply.started":"2024-03-12T10:51:45.927261Z","shell.execute_reply":"2024-03-12T10:51:45.932448Z"},"trusted":true},"execution_count":180,"outputs":[{"execution_count":180,"output_type":"execute_result","data":{"text/plain":"'from langchain.chains.conversation.memory import ConversationBufferWindowMemory\\n\\nconversation = ConversationChain(\\n\\tllm=llm,\\n\\tmemory=ConversationBufferWindowMemory(k=1)\\n)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"from langchain.chains.conversation.memory import ConversationBufferWindowMemory\nfrom langchain.chains import ConversationalRetrievalChain\nretriever=vectordb.as_retriever()\nqa=ConversationalRetrievalChain.from_llm(\n    llm,\n    retriever=retriever,\n    memory=memory)#ConversationBufferWindowMemory(k=1,memory_key=\"chat_history\",return_messages=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:51:47.634158Z","iopub.execute_input":"2024-03-12T10:51:47.634472Z","iopub.status.idle":"2024-03-12T10:51:47.640185Z","shell.execute_reply.started":"2024-03-12T10:51:47.634449Z","shell.execute_reply":"2024-03-12T10:51:47.639282Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"code","source":"q1=\"what is RAG?\"\nresult=qa({\"question\":q1})\nresult[\"answer\"]","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:51:48.955379Z","iopub.execute_input":"2024-03-12T10:51:48.955764Z","iopub.status.idle":"2024-03-12T10:51:52.409543Z","shell.execute_reply.started":"2024-03-12T10:51:48.955739Z","shell.execute_reply":"2024-03-12T10:51:52.408703Z"},"trusted":true},"execution_count":182,"outputs":[{"execution_count":182,"output_type":"execute_result","data":{"text/plain":"'RAG stands for Retrieval-Augmented Generation. It is a framework that provides Language Model (LLM) powered applications access to external data sources and applications that were not seen during training. RAG allows the model to have access to additional data beyond what it learned during pretraining and fine-tuning, improving model completion relevance and mitigating challenges like knowledge limitations and hallucinations.'"},"metadata":{}}]},{"cell_type":"code","source":"q2=\"explain Transformers?\"\nresult=qa({\"question\":q2})\nresult[\"answer\"]","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:52:01.083373Z","iopub.execute_input":"2024-03-12T10:52:01.083719Z","iopub.status.idle":"2024-03-12T10:52:04.515214Z","shell.execute_reply.started":"2024-03-12T10:52:01.083693Z","shell.execute_reply":"2024-03-12T10:52:04.514393Z"},"trusted":true},"execution_count":183,"outputs":[{"execution_count":183,"output_type":"execute_result","data":{"text/plain":"'Transformers are a type of neural network architecture that was introduced in 2017 and are at the core of many modern language models. They are used in various language tasks, such as generating completions for input prompts, gaining contextual understanding from training data, and creating personalized content. Transformers consist of components like embeddings, self-attention layers, encoders, decoders, and softmax outputs, and they are essential for tasks like text classification and generative tasks. Transformers have different variants, including encoder-only, decoder-only, and encoder-decoder models, each trained with different objectives to address various generative tasks.'"},"metadata":{}}]},{"cell_type":"code","source":"q3=\"explain its architecture \"\nresult=qa({\"question\":q3})\nresult[\"answer\"]","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:52:17.859336Z","iopub.execute_input":"2024-03-12T10:52:17.859707Z","iopub.status.idle":"2024-03-12T10:52:40.896885Z","shell.execute_reply.started":"2024-03-12T10:52:17.859680Z","shell.execute_reply":"2024-03-12T10:52:40.895855Z"},"trusted":true},"execution_count":184,"outputs":[{"execution_count":184,"output_type":"execute_result","data":{"text/plain":"'The architecture of Transformers consists of components such as input token context window, embeddings, encoder, self-attention layers, decoder, and softmax output. The model generates new tokens in a loop until a stop condition is reached, often signaled by an end-of-sequence (EOS) token. The Transformer architecture is a key component in generative models, particularly in language-related tasks.'"},"metadata":{}}]},{"cell_type":"code","source":"q4=\"explain chain of thoughts?\"\nresult=qa({\"question\":q4})\nresult[\"answer\"]","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:53:04.681970Z","iopub.execute_input":"2024-03-12T10:53:04.682280Z","iopub.status.idle":"2024-03-12T10:53:27.606606Z","shell.execute_reply.started":"2024-03-12T10:53:04.682257Z","shell.execute_reply":"2024-03-12T10:53:27.605777Z"},"trusted":true},"execution_count":185,"outputs":[{"execution_count":185,"output_type":"execute_result","data":{"text/plain":"'A chain of thoughts is a reasoning strategy that involves breaking down a problem or task into sequential steps or thought processes. Each step in the chain builds upon the previous one, leading to a logical progression towards a solution or understanding of the issue at hand. This method helps guide the thought process systematically and can be used to prompt large language models to elicit reasoning and generate accurate responses.'"},"metadata":{}}]},{"cell_type":"code","source":"q5=\"give example of code\"\nresult=qa({\"question\":q5})\nresult[\"answer\"]","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:53:27.608099Z","iopub.execute_input":"2024-03-12T10:53:27.608388Z","iopub.status.idle":"2024-03-12T10:54:13.724056Z","shell.execute_reply.started":"2024-03-12T10:53:27.608361Z","shell.execute_reply":"2024-03-12T10:54:13.723276Z"},"trusted":true},"execution_count":186,"outputs":[{"execution_count":186,"output_type":"execute_result","data":{"text/plain":"'Sure, here is an example of code for implementing the Transformer architecture using the Hugging Face Transformers Python library:\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\\n\\n# Load the pre-trained model and tokenizer\\nmodel = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\\ntokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\\n\\n# Define training arguments\\ntraining_args = TrainingArguments(\\n    output_dir=\"./results\",  # output directory\\n    num_train_epochs=3,  # number of training epochs\\n    per_device_train_batch_size=2,  # batch size per device during training\\n    save_steps=10_000,  # save checkpoint every 10,000 steps\\n    save_total_limit=2,  # limit the total amount of saved checkpoints\\n)\\n\\n# Define trainer\\ntrainer = Trainer(\\n    model=model,  # the instantiated ü§ó Transformers model to be trained\\n    args=training_args,  # training arguments\\n    train_dataset=dataset,  # training dataset\\n)\\n\\n# Train the model\\ntrainer.train()\\n```\\n\\nThis code snippet demonstrates how to load a pre-trained Transformer model (in this case, DistilGPT2), define training arguments, and train the model using the Trainer class from the Hugging Face Transformers library. Remember to replace \"distilgpt2\" with the specific model you want to use.'"},"metadata":{}}]},{"cell_type":"markdown","source":"from pinecone import Pinecone as PineconeClient","metadata":{"execution":{"iopub.status.busy":"2024-03-12T08:06:15.342762Z","iopub.execute_input":"2024-03-12T08:06:15.343091Z","iopub.status.idle":"2024-03-12T08:06:15.451489Z","shell.execute_reply.started":"2024-03-12T08:06:15.343066Z","shell.execute_reply":"2024-03-12T08:06:15.450667Z"}}},{"cell_type":"markdown","source":"from langchain.vectorstores import Chroma, Pinecone\nfrom pinecone import Pinecone\nPinecone(\n    api_key=secret_value_PC,  # find at app.pinecone.io\n    environment=secret_value_P_ENV  # next to api key in console\n )\nindex_name = \"ragbot\" # put in the name of your pinecone index here\n","metadata":{"execution":{"iopub.status.busy":"2024-03-12T09:23:55.438354Z","iopub.execute_input":"2024-03-12T09:23:55.439016Z","iopub.status.idle":"2024-03-12T09:23:55.444568Z","shell.execute_reply.started":"2024-03-12T09:23:55.438981Z","shell.execute_reply":"2024-03-12T09:23:55.443893Z"}}},{"cell_type":"markdown","source":"index = Pinecone.Index(index_name)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T09:36:19.364115Z","iopub.execute_input":"2024-03-12T09:36:19.364482Z","iopub.status.idle":"2024-03-12T09:36:19.399186Z","shell.execute_reply.started":"2024-03-12T09:36:19.364454Z","shell.execute_reply":"2024-03-12T09:36:19.397810Z"}}},{"cell_type":"markdown","source":"docsearch = Pinecone.from_texts([t.page_content for t in splits], embeddings, index_name=index_name)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-12T09:29:17.035725Z","iopub.execute_input":"2024-03-12T09:29:17.036051Z","iopub.status.idle":"2024-03-12T09:29:17.061348Z","shell.execute_reply.started":"2024-03-12T09:29:17.036026Z","shell.execute_reply":"2024-03-12T09:29:17.060446Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}